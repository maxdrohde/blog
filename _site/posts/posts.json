[
  {
    "path": "posts/2022-01-16-oddsratiodichotomize/",
    "title": "Sensitivity of odds-ratios calculated on dichotomized variables to inclusion criteria",
    "description": "A short simulation example showing why dichomization of continuous variables can lead to wrong conclusions.",
    "author": [
      {
        "name": "Maximilian Rohde",
        "url": "https://maximilianrohde.com"
      }
    ],
    "date": "2022-01-16",
    "categories": [],
    "contents": "\n\n\nlibrary(tidyverse)\nset.seed(123)\n\n\n\nMotivation\nIn this document, we will show how calculating an odds ratio based on\na dichotomized continuous predictor variable can be manipulated by\nchanging the range of the predictor variable that was sampled (i.e,\nstudy inclusion criteria), whereas a logistic regression model that uses\nthe continuous values of the predictor will produce a stable\nestimate.\nScenario\nAssume that we are interested in a disease where the incidence varies\nwith age.\nWe will assume as the true model a simple relationship where the\nprobability of developing the disease is a linear function of age. The\nbelow plot shows this relationship.\n\n\n# True model of disease probability is a linear function of age\np_disease <- function(age){\n  0.25 + 0.0075*age\n}\n\n# Plot true model\ntibble(age = seq(20, 80, length.out=2), prob = p_disease(age)) %>%\n  ggplot() +\n  aes(x=age, y=prob) +\n  geom_line() +\n  labs(title = \"True probability of having the disease\",\n       x=\"Age\",\n       y=\"P(Disease)\") +\n  theme_bw()\n\n\n\n\nWe decide to sample subjects from the population and record if they\nhave the disease. For simplicity, assume we sample patients uniformly\nwithin a given age range. We will show that dichotomizing age at a\ncutpoint is not a good idea, and can lead to estimates that can be\ngreatly affected by the chosen age range to be sampled.\nTo dichotomize the predictor variable, let’s compare the incidence of\ndisease among old (age > 50) and young (age < 50) patients and\ncalculate an odds ratio, instead of using age as a continuous variable.\nThe below simulation shows the results of two scenarios. As a\ncomparison, we also fit a logistic regression using continuous age.\nFirst, we sample 10,000 subjects with ages between 40 and 60. Second,\nwe sample 10,000 subjects with ages between 20 and 80. We show that the\nchoices of inclusion criteria has a large effect on the odds ratio\ncomparing odds of disease between young and old subjects, but the\nestimates provided by logistic regression are unchanged.\nSimulation\nSample from ages 40 to 60\n\n\n# Draw 10,000 patients uniformly between 40 and 60\nages <- runif(10000, min=40, max=60)\n\n# Calculate true probabilities for each patient\nprobs <- p_disease(ages)\n\n# Generate data where each patient has `probs` probability of having the disease\ndata <- map_dbl(probs, ~sample(c(0,1), size=1, prob=c(1-.x, .x)))\n\n\n\n\n\n# Put simulation data into a data frame\ndf <- tibble(age=ages, prob=probs, disease=data)\n\n# Dichotomize at age = 50\ndf$old <- (df$age > 50)\n\nhead(df)\n\n\n# A tibble: 6 × 4\n    age  prob disease old  \n  <dbl> <dbl>   <dbl> <lgl>\n1  45.8 0.593       1 FALSE\n2  55.8 0.668       1 TRUE \n3  48.2 0.611       0 FALSE\n4  57.7 0.682       1 TRUE \n5  58.8 0.691       1 TRUE \n6  40.9 0.557       1 FALSE\n\n\n\ntable(df$disease, df$old)\n\n\n   \n    FALSE TRUE\n  0  2081 1614\n  1  2976 3329\n\n\n\nodds_ratio <- (3329 / 1614) / (2976 / 2081)\n\nodds_ratio\n\n\n[1] 1.442279\n\n\n\n# Fit logistic regression model using continuous age\nglm(disease ~ age, family=binomial(), data=df)\n\n\n\nCall:  glm(formula = disease ~ age, family = binomial(), data = df)\n\nCoefficients:\n(Intercept)          age  \n   -1.23211      0.03547  \n\nDegrees of Freedom: 9999 Total (i.e. Null);  9998 Residual\nNull Deviance:      13170 \nResidual Deviance: 13080    AIC: 13080\n\nSample from ages 20 to 80\n\n\nages <- runif(10000, min=20, max=80)\nprobs <- p_disease(ages)\ndata <- map_dbl(probs, ~sample(c(0,1), size=1, prob=c(1-.x, .x)))\ndf <- tibble(age=ages, prob=probs, disease=data)\ndf$old <- (df$age > 50)\n\ntable(df$disease, df$old)\n\n\n   \n    FALSE TRUE\n  0  2400 1179\n  1  2625 3796\n\n\n\nodds_ratio <- (3796 / 1179) / (2625 / 2400)\n\nodds_ratio\n\n\n[1] 2.943705\n\n\n\nglm(disease ~ age, family=binomial(), data=df)\n\n\n\nCall:  glm(formula = disease ~ age, family = binomial(), data = df)\n\nCoefficients:\n(Intercept)          age  \n   -1.15573      0.03586  \n\nDegrees of Freedom: 9999 Total (i.e. Null);  9998 Residual\nNull Deviance:      13040 \nResidual Deviance: 12220    AIC: 12230\n\nConclusion\nWe see that when sampling from ages 40 to 60, the dichotomization\napproach estimated an odds ratio of 1.44 compared to an odds ratio of\n2.94 when sampling from ages 20 to 80.\nIn contrast, when sampling from ages 40 to 60, the logistic\nregression estimated a regression coefficient for age of 0.0355 compared\nto a very similar value of 0.0359 when sampling from ages 20 to 80.\n\n\n\n",
    "preview": "posts/2022-01-16-oddsratiodichotomize/oddsratiodichotomize_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-03-31T11:13:26-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-10-bs4/",
    "title": "Creating a bookdown book with the bs4 theme",
    "description": "A short guide to creating a bookdown book using the bs4 theme",
    "author": [
      {
        "name": "Maximilian Rohde",
        "url": "https://maximilianrohde.com"
      }
    ],
    "date": "2021-08-10",
    "categories": [],
    "contents": "\nWhat is the bs4 theme?\nIf you’ve been learning R through the amazing free resources made available by the R community, then you probably seen examples of the bs4 bookdown theme in a few of these various sources:\nR for Data Science: https://r4ds.had.co.nz\nThe ggplot2 book: https://ggplot2-book.org\nMastering Shiny: https://mastering-shiny.org\nI think the bs4 theme looks better, and is easier to navigate, than the older gitbook theme (example here).\nThis is a quick guide on how to get started. For comprehensive details, look at the official guide by Yihui Xie, the author of bookdown.\nHow do you use the bs4 theme?\nTo follow this tutorial, you need to have the development version of bookdown installed. You can install it with\n\n\nremotes::install_github('rstudio/bookdown')\n\n\n\nIt will also be helpful to work in RStudio, although it is not strictly necessary.\nWe will create an example project using the bs4 theme, using a template provided by the developers. Then we can edit this template project to make our bookdown book. To create an example project, enter the following command into the R console within RStudio:\n\n\nbookdown::create_bs4_book(\"your_project_name_here\")\n\n\n\nThis will create the project in your current directory.\nNow, click on the .Rproj file within the project directory to open the project in RStudio. You will see the files of the example like this:\nExamples bs4 filesYou can then edit the files according to your needs. The first file to start editing is index.rmd, which contains the metadata of the book such as the title, author, date and more. It also contains the content that will go on the landing page for the HTML rendering of the book.\nNext you will want to edit files for each chapter, and delete those you don’t need.\nLastly, to fiddle with the theme, you can edit the style.css file if you have some experience with CSS.\nTo render the book, use\n\n\nbookdown::render_book()\n\n\n\nThen to view the preview of the HTML page, use\n\n\nbookdown::serve_book()\n\n\n\nThe final resultTo host my bookdown books online, I put the project into a GitHub repository, and then use Netlify to host it for free. However, this is out of the scope of this tutorial. The official guide by Yihui Xie has more details on the hosting process.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-10T14:23:04-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-16-graddescpt1/",
    "title": "What is Gradient Descent? (Part I)",
    "description": "Exploring gradient descent using R and a minimal amount of mathematics",
    "author": [
      {
        "name": "Maximilian Rohde",
        "url": "https://maximilianrohde.com"
      }
    ],
    "date": "2021-01-16",
    "categories": [],
    "contents": "\n\n\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(gganimate)\n\n\n\nIntroduction\nGradient descent is an optimization algorithm that finds the minimum of a function. Commonly, the function to be minimized is a loss function: a function that quantifies the “badness” associated with the given inputs, which you would naturally want to minimize. A common loss function is the mean-squared error. For example, using mean-squared error, the loss incurred by an inaccurate prediction is the squared distance from the prediction to the true value. Neural networks are commonly optimized using some form of gradient descent.\nLet’s start with a simple example, where we already know the answer. We wish to minimize the quadratic function given by\n\\[\nf(x) = (x + 2)^2 + 3\n\\] The shape of the function, a parabola, is shown in the plot below. Most applications of gradient descent occur in dimensions much higher than 2D, where we cannot so easily visualize the function we are trying to minimize.\n\n\nShow code\n\nx <- seq(-10,5,length.out=1e4)\ny <- (x + 2)^2 + 3\n\nplot_data <- tibble(x, y)\n\nplot_data %>%\n  ggplot() +\n  aes(x=x, y=y) +\n  geom_line() +\n  geom_hline(yintercept = 3, linetype=2) +\n  geom_vline(xintercept = -2, linetype=2) +\n  scale_x_continuous(breaks=c(-2)) +\n  scale_y_continuous(breaks=c(3)) +\n  cowplot::theme_cowplot(font_family = \"Lato\")\n\n\n\n\nFinding the minimum is a solved problem using calculus. We can take the first-derivative, set it equal to zero, and solve, to obtain a minimum of \\(y=3\\), which occurs at \\(x=-2\\). Then an application of the second-derivative test confirms that it is minimum, rather than a maximum. Our goal is to reproduce this result using gradient descent.\nAn analogy for gradient descent\nGradient descent works by starting at a location in the space. Then, each iteration of the algorithm it moves downhill with respect to the function, which is by definition opposite the gradient. The algorithm proceeds downhill until it reaches a minimum where the gradient is zero within some tolerance (success) or the maximum number of iterations is reached (failure). Apart from the tuning parameters of the algorithm, which we will discuss later, the only information gradient descent needs to work is the function to be minimized and its first derivative.\nHere’s an analogy. Think of a ball moving under the influence of gravity in a landscape of hills and valleys. If you let the ball move freely, it will roll to a point of minimum height in the landscape. Does the ball know the whole landscape and decide to move to the minimum point? No. The only information it uses to find the minimum is the slope at the point it is currently at. The local information is enough. Gravity is constantly moving the ball downhill, based on the slope of the landscape at the current location.\nThe gradient descent algorithm\nThe general algorithm for gradient descent is as follows:\nPick a starting point and a learning rate\nUsing the derivative of the function, compute the gradient (i.e., slope) at the current point.\nCompute the step size: \\(\\text{delta} = - \\text{gradient} * \\text{learning_rate}\\)\nSet \\(x \\rightarrow x + \\text{delta}\\)\nRepeat from step 2 until either delta is below a certain threshold or a maximum number of iterations is reached\nNow, let’s go through this step-by-step for our quadratic function example.\nPick an arbitrarily chosen starting point of \\(x=5\\). Thus \\(f(x) = (5 + 2)^2 + 3 = 52\\). We also pick a commonly used learning rate of 0.1.\nThe derivative of \\(f(x)\\) is \\(\\frac{df}{dx} = 2x+4\\). So the gradient is \\(2(5) + 4 = \\boxed{14}\\)\nSet the step size: \\(\\text{delta} = - \\text{gradient} * \\text{learning_rate} = - 14* 0.01 = \\boxed{-0.14}\\)\nSet the current value of \\(x\\) to \\(x + delta = 5 - 0.14 = \\boxed{4.86}\\)\nAssume we set the step size (delta) threshold to 0.001 and the maximum number of iteration to 5000. Since neither of these criteria are currently met, we go back to step 2, but now with \\(x=4.86\\), and repeat until we meet one of the exit conditions.\nImplementation in R\nNow that we understand the gradient descent algorithm in theory, let’s translate this into R code.\n\n\n# Define f(x) and df/dx\nf <- function(x){(x+2)^2 + 3}\ndf_dx <- function(x){2*x+4}\n\n# Set learning rate\nlearning_rate <- 0.1\n\n# Set starting point\nx <- 5\n\n# Create a counter to track the iteration\niter <- 1\n\nwhile(TRUE){ # Loop until we reach exit conditions\n  \n  # Compute the gradient at the current x value\n  current_grad <- df_dx(x)\n  \n  # Compute delta at the current x value\n  delta <- -current_grad*learning_rate\n  \n  # Compute the updated x value, given delta\n  x <- x + delta\n  \n  # Print the current state of the algorithm\n  # the glue package is used for printing variables easily\n  print(glue(\"Iteration: {iter}\"))\n  print(glue(\"x: {x}\"))\n  print(glue(\"y: {f(x)}\"))\n  print(glue(\"delta: {delta}\"))\n  \n  # Increment the iteration counter\n  iter <- iter + 1\n  \n  # Exit if delta is below the threshold or max iterations have been reached\n  if (abs(delta)<0.001 | iter>5000) {\n    break\n  }\n}\n\n\n\nHere’s the output from the beginning and end of the algorithm.\nIteration: 1\nx: 3.6\ny: 34.36\ndelta: -1.4\nIteration: 2\nx: 2.48\ny: 23.0704\ndelta: -1.12\nIteration: 3\nx: 1.584\ny: 15.845056\ndelta: -0.896\n\n...\n\nIteration: 32\nx: -1.994454028624\ny: 3.0000307577985\ndelta: -0.00138649284399963\nIteration: 33\nx: -1.9955632228992\ny: 3.00001968499104\ndelta: -0.00110919427519969\nIteration: 34\nx: -1.99645057831936\ny: 3.00001259839427\ndelta: -0.000887355420159741\n\nWe see that our gradient descent algorithm converged to the minimum at \\((3,-2)\\), with some error that could be reduced if we lowered the step size threshold.\nAnimations\nAnimations are a good way to get intuition on how optimization algorithms like gradient descent work. The animation below shows our algorithm using three different learning rates. The code to create the animations can be found here. Note: The animations may not display well on some mobile devices.\n\n\n  \n\nUsing a learning rate of 0.01 takes much longer to converge, but with more complicated functions it is less likely to overshoot and miss the minimum. Using a learning rate of 0.95, the algorithm constantly overshoots the minimum and oscillates on either side of it until it finally settles down. A learning rate of 0.1 seems like the best compromise between accuracy and speed, since we know the true minimum.\nIn this specific example, a learning rate higher than 1 will constantly overshoot the minimum and will never converge. A special case is when the learning rate is exactly 1: the algorithm will move between two points on opposite sides on the parabola and remain stuck there. See the animation below.\n\n  In real applications, where the true minimum is unknown, trial and error is necessary to find a good learning rate. There are more complex algorithms that build on gradient descent to automatically tune the learning rate as the algorithm progresses.\nNext steps\nIn the next post in this series, we will extend our gradient descent algorithm to optimize over more complex functions: fitting a least-squares regression line, and a logistic regression curve.\n\n\n\n",
    "preview": "posts/2021-01-16-graddescpt1/graddescpt1_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-01-17T18:51:09-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-13-robustestimators/",
    "title": "Statistical simulation of robust estimators with tidyverse tools",
    "description": "Tidyverse tools provide a powerful way to do statistical simulations. We demonstrate this approach by evaluating the properties of the mean and median as estimators of center for two distributions.",
    "author": [
      {
        "name": "Maximilian Rohde",
        "url": "https://maximilianrohde.com"
      }
    ],
    "date": "2021-01-13",
    "categories": [],
    "contents": "\n\n\n# Load packages\nlibrary(tidyverse)\n\n# ggplot2 themes\nlibrary(cowplot)\n\n# Formatting of HTML tables\nlibrary(kableExtra)\n\nset.seed(7)\n\n\n\nThe sample mean and sample median are commonly used estimators for the center of distribution. There is no such thing as a “best estimator” in all circumstances. However, estimators can definitely outperform other estimators in terms of desirable properties (e.g., unbiasedness, low variance, consistency) given a particular circumstance. We can use simulation and mathematical theory to evaluate to performance of estimators given a set of assumptions. Here we focus on using simulation, with the help of tools from the tidyverse.\nScenario 1\nWe begin with a simple example. The true data generating process is \\[ X_1, X_2, \\ldots X_n \\stackrel{iid}{\\sim} N(3,1) \\]\nHow will the mean and median perform as estimators of center? Let’s use simulation to find out.\nWe will use a tibble to store all of our simulation results. First, let’s decide what sample sizes to simulate, and how many trials to run. The more trials we run, the more accurate our simulation results will be – the only cost being increased time to run the simulations.\n\n\nsize <- c(5,10,20,50,100,200)\ntrial <- 1:1e5\n\n\n\nNow we use crossing() to generate a tibble that contains every combination of the vectors size and trial. So for every sample size, we are repeating it 100,000 times.\n\n\ndf <- crossing(trial, size)\n\n\n\n\n# A tibble: 600,000 x 2\n   trial  size\n   <int> <dbl>\n 1     1     5\n 2     1    10\n 3     1    20\n 4     1    50\n 5     1   100\n 6     1   200\n 7     2     5\n 8     2    10\n 9     2    20\n10     2    50\n# … with 599,990 more rows\n\nNow in each row, we want a unique sample of data with the sample size given by that row. We will use purrr::map() to do this.\n\n\ndf$data <- map(df$size, ~rnorm(n=.x, mean = 3, sd=1))\n\n\n\nThe first argument to map is the vector to iterate over, and the second argument is the function to apply. We use .x as a dummy variable to refer to the value in the current iteration. In words, we are mapping each sample size to a random sample of size \\(n\\) from a normal distribution.\nThe new column, data, is a vector of lists, where each list contains a unique sample of data. Let’s see what this looks like.\n\n# A tibble: 600,000 x 3\n   trial  size data       \n   <int> <dbl> <list>     \n 1     1     5 <dbl [5]>  \n 2     1    10 <dbl [10]> \n 3     1    20 <dbl [20]> \n 4     1    50 <dbl [50]> \n 5     1   100 <dbl [100]>\n 6     1   200 <dbl [200]>\n 7     2     5 <dbl [5]>  \n 8     2    10 <dbl [10]> \n 9     2    20 <dbl [20]> \n10     2    50 <dbl [50]> \n# … with 599,990 more rows\n\nNow that we have our data, we can compute the mean and median for each sample.\n\n\ndf$mean <- map_dbl(df$data, ~mean(.x))\ndf$median <- map_dbl(df$data, ~median(.x))\n\n\n\n\n# A tibble: 600,000 x 5\n   trial  size data         mean median\n   <int> <dbl> <list>      <dbl>  <dbl>\n 1     1     5 <dbl [5]>    2.80   2.31\n 2     1    10 <dbl [10]>   3.96   3.55\n 3     1    20 <dbl [20]>   3.14   3.15\n 4     1    50 <dbl [50]>   3.07   3.09\n 5     1   100 <dbl [100]>  3.16   3.32\n 6     1   200 <dbl [200]>  2.96   2.95\n 7     2     5 <dbl [5]>    2.52   2.78\n 8     2    10 <dbl [10]>   2.83   2.80\n 9     2    20 <dbl [20]>   3.08   3.14\n10     2    50 <dbl [50]>   2.93   2.86\n# … with 599,990 more rows\n\nThe mean and median of each sample are now in separate columns. However, to get the data into tidy format, also known as long format, we want them in separate rows. Having the data in tidy format allows us to use ggplot2 and other tidyverse functions more effectively. We use pivot_longer to do this.\n\n\ndf <- pivot_longer(df, cols=mean:median, names_to=\"Estimator\", values_to=\"Estimate\")\n\n\n\n\n# A tibble: 1,200,000 x 5\n   trial  size data        Estimator Estimate\n   <int> <dbl> <list>      <chr>        <dbl>\n 1     1     5 <dbl [5]>   mean          2.80\n 2     1     5 <dbl [5]>   median        2.31\n 3     1    10 <dbl [10]>  mean          3.96\n 4     1    10 <dbl [10]>  median        3.55\n 5     1    20 <dbl [20]>  mean          3.14\n 6     1    20 <dbl [20]>  median        3.15\n 7     1    50 <dbl [50]>  mean          3.07\n 8     1    50 <dbl [50]>  median        3.09\n 9     1   100 <dbl [100]> mean          3.16\n10     1   100 <dbl [100]> median        3.32\n# … with 1,199,990 more rows\n\nNow we are finally ready to analyze the results of our simulation. First, let’s compute the bias and variance of our estimators for each sample size.\n\n\ndf %>%\n  group_by(size, Estimator) %>%\n  summarize(Bias = (mean(Estimate) - 3),\n            Variance = var(Estimate)) %>%\n  pivot_longer(Bias:Variance) -> summary_df\n\n\n\n\n# A tibble: 24 x 4\n# Groups:   size [6]\n    size Estimator name          value\n   <dbl> <chr>     <chr>         <dbl>\n 1     5 mean      Bias      0.0000212\n 2     5 mean      Variance  0.199    \n 3     5 median    Bias      0.00141  \n 4     5 median    Variance  0.286    \n 5    10 mean      Bias     -0.000959 \n 6    10 mean      Variance  0.100    \n 7    10 median    Bias     -0.000314 \n 8    10 median    Variance  0.138    \n 9    20 mean      Bias      0.000634 \n10    20 mean      Variance  0.0502   \n# … with 14 more rows\n\nPlotting the bias and variance as a function of sample size, we see that both the mean and median are unbiased estimators of the center of the true distribution, but the median has higher variance. Therefore, we would prefer the mean under these assumptions.\n\n\nShow code\n\nsummary_df %>%\n  ggplot() +\n  aes(x=size, y=value, color=Estimator) +\n  geom_line(alpha=0.6) +\n  geom_point(alpha=0.6) +\n  facet_wrap(~name) +\n  scale_color_brewer(palette = \"Set1\") +\n  cowplot::theme_cowplot(font_size = 12, font_family = \"Lato\") +\n  theme(legend.position = c(0.8,0.8)) +\n  labs(\n    x = \"Sample Size\",\n    y = \"Estimated Value\"\n  )\n\n\n\n\nPlotting the sampling distribution for each of the estimators shows that the median indeed has higher variance.\n\n\nShow code\n\nlabel_names <- as_labeller(c(`5` = \"Sample Size: 5\",\n                             `10` = \"Sample Size: 10\",\n                             `20` = \"Sample Size: 20\",\n                             `50` = \"Sample Size: 50\",\n                             `100` = \"Sample Size: 100\",\n                             `200` = \"Sample Size: 200\")) \n\ndf %>%\n  ggplot() +\n  aes(x=Estimate, color=Estimator, fill=Estimator) +\n  geom_density(alpha=0.3, size=0.8) +\n  facet_wrap(~size, labeller=label_names) +\n  geom_vline(aes(xintercept = 3), linetype=2, alpha=0.3) +\n  coord_cartesian(xlim=c(1,5), ylim=c(0,6)) +\n  cowplot::theme_cowplot(font_family = \"Lato\", font_size=12) +\n  scale_fill_brewer(palette = \"Set1\") +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = c(0.88,0.88)) +\n  labs(\n    title = \"Normal(3,1) Distribution\",\n    subtitle = \"Performance of mean and median\",\n    x=\"Estimate\",\n    y=\"PDF\"\n  )\n\n\n\n\nScenario 2\nNow let’s take a look at a distribution with heavier tails than the normal. An example is a mixture of normal two distributions.\nThe data-generating process is this: With probability 0.9, draw from the \\(N(3, 1)\\) distribution. Otherwise, (with probability 0.1), draw from the \\(N(3, 10)\\) distribution.\nWe can write a function to draw from this distribution.\n\n\n# generates 1 draw from the specifies mixture normal distribution\nmixed_normal <- function(){\n  x <- runif(1)\n  if (x>0.1) {\n    return(rnorm(n=1, mean = 3, sd=1))\n  }\n  else{\n    return(rnorm(n=1, mean = 3, sd=10))\n  }\n}\n\n# generates n draws from the specifies mixture normal distribution\nrmixed_norm  <- function(n){\n  map_dbl(1:n, ~mixed_normal())\n}\n\n\n\nPlotting the normal distribution and the mixture distribution on top of each other, we see that they are very similar, but the mixture distribution has heavier tails (i.e., more of the probability mass is in the tails compared to the normal distribution).\n\n\nShow code\n\ntibble(normal = rnorm(1e5, mean=3),\n       mixture = rmixed_norm(1e5)) %>%\n  pivot_longer(cols=normal:mixture, names_to=\"Distribution\", values_to=\"value\") %>%\n  ggplot() +\n  aes(x=value, color=Distribution) +\n  geom_density(alpha=0.7)+\n  cowplot::theme_cowplot(font_family = \"Source Sans Pro\", font_size=12) +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = c(0.88,0.90)) +\n  labs(\n    title = \"Mixture Normal vs Normal Distribution\",\n    subtitle = \"\",\n    x= \"X\",\n    y= \"PDF\"\n  )\n\n\n\n\nNow let’s compare the performance of the mean and median on the mixture distribution.\n\n\nShow code\n\nsize <- c(5,10,20,50,100,200)\ntrial <- 1:1e5\n\ndf <- crossing(trial, size)\n\ndf$data <- map(df$size, ~rmixed_norm(n=.x))\n\ndf$mean <- map_dbl(df$data, ~mean(.x))\ndf$median <- map_dbl(df$data, ~median(.x))\n\ndf <- pivot_longer(df, cols=mean:median, names_to=\"Estimator\", values_to=\"Estimate\")\n\ndf %>%\n  group_by(size, Estimator) %>%\n  summarize(Bias = (mean(Estimate) - 3),\n            Variance = var(Estimate)) %>%\n  pivot_longer(Bias:Variance) -> summary_df\n\n\nsummary_df %>%\n  ggplot() +\n  aes(x=size, y=value, color=Estimator) +\n  geom_line(alpha=0.6) +\n  geom_point(alpha=0.6) +\n  facet_wrap(~name) +\n  scale_color_brewer(palette = \"Set1\") +\n  cowplot::theme_cowplot(font_size = 12, font_family = \"Lato\") +\n  theme(legend.position = c(0.8,0.8)) +\n  labs(\n    x = \"Sample Size\",\n    y = \"Estimated Value\"\n  )\n\n\n\nShow code\n\nlabel_names <- as_labeller(c(`5` = \"Sample Size: 5\",\n                             `10` = \"Sample Size: 10\",\n                             `20` = \"Sample Size: 20\",\n                             `50` = \"Sample Size: 50\",\n                             `100` = \"Sample Size: 100\",\n                             `200` = \"Sample Size: 200\")) \n\ndf %>%\n  ggplot() +\n  aes(x=Estimate, color=Estimator, fill=Estimator) +\n  geom_density(alpha=0.3, size=0.8) +\n  facet_wrap(~size, labeller=label_names) +\n  geom_vline(aes(xintercept = 3), linetype=2, alpha=0.3) +\n  coord_cartesian(xlim=c(1,5), ylim=c(0,6)) +\n  cowplot::theme_cowplot(font_family = \"Lato\", font_size=12) +\n  scale_fill_brewer(palette = \"Set1\") +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = c(0.88,0.88)) +\n  labs(\n    title = \"Mixture Normal Distribution\",\n    subtitle = \"Performance of mean and median\",\n    x=\"Estimate\",\n    y=\"PDF\"\n  )\n\n\n\n\nIt looks like the median greatly outperforms the mean! Both are unbiased, but the median has lower variance.\nTake-away points\nSimulation is a powerful tool in statistics. Here we showed how is can be used to compare the properties of estimators.\nFor distributions with heavy tails, the median may be a better estimator of center than the mean.\n\n\n\n",
    "preview": "posts/2021-01-13-robustestimators/robustestimators_files/figure-html5/unnamed-chunk-13-1.png",
    "last_modified": "2021-01-17T02:30:08-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-06-gganimatemap/",
    "title": "Using gganimate with ggmap",
    "description": "Combining gganimate with ggmap can be used to create animations of geographic data. I give a few examples using data from Nashville Open Data.",
    "author": [
      {
        "name": "Maximilian Rohde",
        "url": "https://maximilianrohde.com"
      }
    ],
    "date": "2021-01-06",
    "categories": [],
    "contents": "\n\n\nlibrary(tidyverse)\nlibrary(ggmap)\nlibrary(gganimate)\n\n# ggplot2 themes\nlibrary(cowplot)\n\n# Formatting of HTML tables\nlibrary(kableExtra)\n\n\n\nOverview\nAnimating your ggplot2 visualizations is easy using the gganimate package. But did you also know that gganimate can be used with the ggmap package to animate geographic data? Using data from Nashville Open Data, we’ll create an animation to visualize the spatial evolution of parks in Nashville over time.\nData cleaning and exploration\nFirst, let’s load in the data. We can load the data directly from the URL using read_csv().\n\n\n# Read in data\ndf <- read_csv(\"https://data.nashville.gov/resource/74d7-b74t.csv\")\n\n\n\nLet’s take a look at the first couple rows.\n\n\nhead(df, 2) %>% \n  # table styling for HTML output\n  kable(format = \"html\") %>%\n  kable_styling(\"striped\") %>%\n  scroll_box(width = \"100%\")\n\n\n\n\npark_name\n\n\nacres\n\n\nyear_established\n\n\ncommunity_center\n\n\nnature_center\n\n\nplayground\n\n\nada_accessible\n\n\nrestrooms_available\n\n\npicnic_shelters\n\n\npicnic_shelters_quantity\n\n\ndog_park\n\n\nbaseball_fields\n\n\nbasketball_courts\n\n\nvolleyball\n\n\nsoccer_fields\n\n\nfootball_multi_purpose_fields\n\n\ntennis_courts\n\n\ndisc_golf\n\n\nskate_park\n\n\nswimming_pool\n\n\nspray_park\n\n\ngolf_course\n\n\nwalk_jog_paths\n\n\nhiking_trails\n\n\nhorse_trails\n\n\nmountain_bike_trails\n\n\nboat_launch\n\n\ncamping_available_by_permit\n\n\nfishing_by_permit\n\n\nlake\n\n\ncanoe_launch\n\n\ncommunity_garden\n\n\nhistoric_features\n\n\nnotes\n\n\nmapped_location\n\n\nParadise Ridge Park\n\n\n98.41\n\n\n2013\n\n\nYes\n\n\nNo\n\n\nYes\n\n\nYes\n\n\nYes\n\n\nYes\n\n\n1\n\n\nNo\n\n\nNo\n\n\nYes\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nYes\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\ncommunity meeting space, gym, playground, walking trail, 1 reservable picnic shelter, outdoor basketball court\n\n\n3000 Morgan Rd Joelton, TN 37080 (36.335583, -86.85995)\n\n\nCrooked Branch Park\n\n\n84.00\n\n\n2012\n\n\nNA\n\n\nNA\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\n0\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNatural area with walking trails\n\n\n116D Ray Ave Lakewood, TN 37138 (36.241016, -86.640834)\n\n\n\nA few basic questions I have about the dataset are the following:\nAre parks duplicated?\n\n\ndf %>%\n  filter(duplicated(park_name)) %>%\n  kable(format = \"html\") %>%\n  kable_styling(\"striped\") %>%\n  scroll_box(width = \"100%\")\n\n\n\n\npark_name\n\n\nacres\n\n\nyear_established\n\n\ncommunity_center\n\n\nnature_center\n\n\nplayground\n\n\nada_accessible\n\n\nrestrooms_available\n\n\npicnic_shelters\n\n\npicnic_shelters_quantity\n\n\ndog_park\n\n\nbaseball_fields\n\n\nbasketball_courts\n\n\nvolleyball\n\n\nsoccer_fields\n\n\nfootball_multi_purpose_fields\n\n\ntennis_courts\n\n\ndisc_golf\n\n\nskate_park\n\n\nswimming_pool\n\n\nspray_park\n\n\ngolf_course\n\n\nwalk_jog_paths\n\n\nhiking_trails\n\n\nhorse_trails\n\n\nmountain_bike_trails\n\n\nboat_launch\n\n\ncamping_available_by_permit\n\n\nfishing_by_permit\n\n\nlake\n\n\ncanoe_launch\n\n\ncommunity_garden\n\n\nhistoric_features\n\n\nnotes\n\n\nmapped_location\n\n\nRiverfront Park\n\n\n5.29\n\n\n1983\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nYes\n\n\nNo\n\n\n0\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nYes\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nYes\n\n\nNo\n\n\nAmphitheater, Pleasure/Commercial Boat Docking\n\n\n100 1st Ave South Nashville, TN (36.162279, -86.774364)\n\n\nRiverfront Park\n\n\n12.54\n\n\n2015\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nYes\n\n\nNo\n\n\nNo\n\n\n0\n\n\nYes\n\n\nNo\n\n\nYes\n\n\nNo\n\n\nNo\n\n\nYes\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nYes\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nOutdoor fitness equipment, dog park, Ascend Amphitheater\n\n\n310 1st Ave S Nashville, TN (36.159082, -86.772376)\n\n\nRiverfront Park\n\n\n3.49\n\n\n1977\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\n0\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nJames Robertson Statue, ,small stair amphitheater\n\n\n170 1st Ave N Nashville, TN (36.164111, -86.77555)\n\n\n\nIt looks like three parks have the name Riverfront Park, but judging by year_established and mapped_location, they are different parks.\nHow many parks are there?\n\n\nnrow(df)\n\n\n[1] 123\n\n123 parks in Nashville – not bad!\nWhich years are represented in the data?\n\n\nrange(df$year_established)\n\n\n[1]    0 2015\n\nStrange, it looks like the oldest park was established in year “0”. This must be a mistake.\n\n\n# Newest 5 parks\nsort(df$year_established) %>% tail()\n\n\n[1] 2013 2013 2014 2014 2015 2015\n\n# Oldest 5 parks\nsort(df$year_established) %>% head()\n\n\n[1]    0 1901 1903 1907 1909 1910\n\nThe true range of the data is 1901 - 2015. It looks like the park recorded as being established in year zero was a mistake. Let’s remove it from the dataset.\n\n\ndf <-\n  df %>%\n  filter(year_established != 0)\n\n\n\nNow let’s take a look at the distribution of the years when parks were established. First, we’ll make a histogram.\n\n\ndf %>%\n  ggplot() +\n  aes(year_established) +\n  geom_histogram(bins=30, color=\"black\", fill=\"grey\") +\n  labs(\n    title = \"Number of parks established in Nashville per year\",\n    subtitle = \"\",\n    x= \"Year Established\",\n    y= \"Frequency\") +\n  cowplot::theme_cowplot(font_family = \"Source Sans Pro\",\n                         font_size = 12)\n\n\n\n\nThe rate of new park development looks to be increasing over time. An ECDF plot supports this observation.\n\n\ndf %>%\n  ggplot()+\n  aes(year_established) +\n  stat_ecdf() +\n  labs(\n    title = \"Cumulative distribution of Nashville parks over time\",\n    subtitle = \"\",\n    x= \"Year Established\",\n    y= \"ECDF\") +\n  cowplot::theme_cowplot(font_family = \"Source Sans Pro\",\n                         font_size = 12)\n\n\n\n\nCreating the Animation\nNow that we’ve cleaned and explored the data, let’s create an animation with gganimate and ggmap. I would like to visualize the spatial evolution of Nashville’s parks by year.\nFirst, let’s try to just plot the locations of all the parks. We see that location is stored in the mapped_location column.\n\n\nhead(df$mapped_location)\n\n\n[1] \"3000 Morgan Rd\\nJoelton, TN 37080\\n(36.335583, -86.85995)\"                 \n[2] \"116D Ray Ave\\nLakewood, TN 37138\\n(36.241016, -86.640834)\"                 \n[3] \"21 Joelton Community Center Rd\\nJoelton, TN 37080\\n(36.316932, -86.870111)\"\n[4] \"1266 Stones River Road\\nHermitage, TN 37076\\n(36.189647, -86.652059)\"      \n[5] \"Grand Ave at 14th Ave\\nNashville, TN 37212\\n(36.1461984, -86.789001)\"      \n[6] \"Snell Blvd at Panorama Dr\\nNashville, TN \\n(36.1797136, -86.8395478)\"      \n\nLuckily, the (latitude, longitude) coordinates are provided, but we need to extract them from the text. We can use a regular expression to do this using the stringr package.\n\n\n# Get a matrix of matched latitudes and longitudes\nmatch <- str_match(df$mapped_location, \"\\\\((.*), (.*)\\\\)\")\n\n# Add the latitudes and longitudes to the data frame\n# data is recorded as character, so need to convert to numeric\ndf$lat <- match[,2] %>% as.numeric()\ndf$long <- match[,3] %>% as.numeric()\n\n\n\nLet’s verify we extracted the coordinates correctly.\n\n\n# View first 5 coordinates\ndf %>%\n  select(lat, long) %>%\n  head() %>%\n  kable(format = \"html\") %>%\n  kable_styling(\"striped\") %>%\n  scroll_box(width = \"100%\")\n\n\n\n\nlat\n\n\nlong\n\n\n36.33558\n\n\n-86.85995\n\n\n36.24102\n\n\n-86.64083\n\n\n36.31693\n\n\n-86.87011\n\n\n36.18965\n\n\n-86.65206\n\n\n36.14620\n\n\n-86.78900\n\n\n36.17971\n\n\n-86.83955\n\n\n\nNow we can make the plot. We start with the qmplot() function from ggmap, which is a shortcut for plotting on maps, just like qplot() in ggplot2. We pass in the latitude and longitude coordinates and the data frame. The argument maptype = \"toner-lite\" indicates the type of basemap to use as the background. We also specify alpha=0.5 so we can see when the points overlap. I would like larger parks to be represented by larger circles, so we can map size to acreage by aes(size=acres). Then we add the extra theming using the cowplot package.\n\n\nqmplot(long, lat, data = df,\n       maptype = \"toner-lite\", alpha=0.5) + \n  aes(size=acres) +\n  labs(\n    title = \"Nashville Parks\",\n    x= \"Longitude\",\n    y= \"Latitude\",\n    caption = \"Area corresponds to acreage \\n Data available from Nashville Open Data\") +\n  cowplot::theme_cowplot(font_family = \"Source Sans Pro\",\n                         font_size = 12) +\n  theme(legend.position = \"none\")\n\n\n\n\nLooks pretty good already. Now let’s make this animated!\nWe will add the transition_states() function from gganimate and specify that each state of the animation is determined by year_established. We also set subtitle = \"Year: {closest_state}\" to display the year of the current frame.\n\n\nqmplot(long, lat, data = df,\n       maptype = \"toner-lite\", alpha=0.5) + \n  aes(size=acres, group=year_established) +\n  labs(\n    title = \"Nashville Parks\",\n    subtitle = \"Year: {closest_state}\",\n    x= \"Longitude\",\n    y= \"Latitude\",\n    caption = \"Area corresponds to acreage \\n Data available from Nashville Open Data\") +\n  cowplot::theme_cowplot(font_family = \"Source Sans Pro\",\n                         font_size = 12) +\n  theme(legend.position = \"none\") +\n  transition_states(year_established)\n\n\n\n\nIt’s animated now, but there are two problems.\nFirst, the points are disappearing after each year. We can add shadow_mark(color=\"black\") to have the points stay on the plot. We specify that the old points are colored black so that we can color the current points red, to highlight which points were just displayed.\nSecond, the passage of time is not constant. We want to have each frame change in increments of one year. In our current animation, the years are skipping between the years present in the data. To fix this, we convert year_established to a factor, and fill in the missing years.\n\n\ndf$year_established <-\n  df$year_established %>%\n  # convert to factor\n  as.factor() %>%\n  # add extra years\n  fct_expand(1900:2019 %>% as.character) %>%\n  # sort years\n  fct_relevel(1900:2019 %>% as.character)\n\n\n\nNow that we’ve made those changes, let’s try again.\n\n\nqmplot(long, lat, data = df,\n       maptype = \"toner-lite\", alpha=0.5, color=\"red\") + \n  aes(size=acres, group=year_established) +\n  labs(\n    title = \"Nashville Parks\",\n    subtitle = \"Year: {closest_state}\",\n    x= \"Longitude\",\n    y= \"Latitude\",\n    caption = \"Area corresponds to acreage \\n Data available from Nashville Open Data\") +\n  cowplot::theme_cowplot(font_family = \"Source Sans Pro\",\n                         font_size = 12) +\n  theme(legend.position = \"none\") +\n  transition_states(year_established) +\n  shadow_mark(color=\"black\")\n\n\n\n\nLooks good! But wait a second… the animation only goes to 1950. Wasn’t it supposed to go to 2015? This is a little quirk of gganimate. By default, the animation is capped at 100 frames. For the transition_states() animation, by default a single frame is allocated for each state, and another frame is allocated for transitions between states. So 100 frames can represent 50 years of data. The animation is cut short because we have more than 50 years of data.\nLet’s fix this by saving the animation to a variable, and then using the animate() function to increase the number of frames.\n\n\nparks_anim <- \n  qmplot(long, lat, data = df,\n       maptype = \"toner-lite\", alpha=0.5, color=\"red\") + \n  aes(size=acres, group=year_established) +\n  labs(\n    title = \"Nashville Parks\",\n    subtitle = \"Year: {closest_state}\",\n    x= \"Longitude\",\n    y= \"Latitude\",\n    caption = \"Area corresponds to acreage \\n Data available from Nashville Open Data\") +\n  cowplot::theme_cowplot(font_family = \"Source Sans Pro\",\n                         font_size = 12) +\n  theme(legend.position = \"none\") +\n  transition_states(year_established) +\n  shadow_mark(color=\"black\")\n\nanimate(\n  parks_anim,\n  nframes=300, # number of frames to compute\n  )\n\n\n\n\nMy preferred method of rendering the animation is to use ffmpeg, instead of the default GIF renderer, because it creates videos (.mp4) rather than GIFs. You will need to install ffpmeg on your computer separately. Using ffmpeg also allows for finer control over the frame rate of the animation and creates smaller files. I’ll show how to use it below.\nThe animate() function has parameters for duration (total duration in seconds), fps (frames per second), and nframes (total number of frames). You can specify any two. For our case, we give the duration and number of frames, and gganimate figures out the proper frame rate to fit the specified number of frames into the specified number of seconds.\nWe also set res=300 to increase the resolution. This has the side effect of making the font appear larger, so we decrease the font size in the call to theme_cowplot().\nBe warned that this may take a bit of time to animate. Usually, I put my code in a .R file and run it from the terminal, rather than in RStudio, which seems to crash less and render more quickly. Otherwise, when I render the animations in an R Markdown file, I set cache=TRUE in the cell so that I don’t have to render each time I knit the document.\n\n\nparks_anim <- \n  qmplot(long, lat, data = df,\n       maptype = \"toner-lite\", alpha=0.5, color=\"red\") + \n  aes(size=acres, group=year_established) +\n  labs(\n    title = \"Nashville Parks\",\n    subtitle = \"Year: {closest_state}\",\n    x= \"Longitude\",\n    y= \"Latitude\",\n    caption = \"Area corresponds to acreage \\n Data available from Nashville Open Data\") +\n  cowplot::theme_cowplot(font_family = \"Source Sans Pro\",\n                         font_size = 10) +\n  theme(legend.position = \"none\") +\n  transition_states(year_established) +\n  shadow_mark(color=\"black\")\n\nanimate(\n  parks_anim,\n  duration=15, # duration of the animation in seconds\n  nframes=768, # number of frames to compute\n  height = 6,\n  width = 6,\n  units = \"in\",\n  res = 300, # resolution of the output\n  renderer = ffmpeg_renderer() # render to video with ffmpeg\n  )\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-01-06-gganimatemap/preview.jpg",
    "last_modified": "2021-01-07T16:42:47-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-23-early-stopping-inflates-type-i-error/",
    "title": "Repeated testing inflates type I error",
    "description": "Type I error is increased when you test your hypothesis multiple times during the data collection process. Simulations can provide a clear picture of this process.",
    "author": [
      {
        "name": "Maximilian Rohde",
        "url": "https://maximilianrohde.com"
      }
    ],
    "date": "2020-12-23",
    "categories": [],
    "contents": "\nWhat’s the problem with repeated testing?\nDoes your astrological sign affect your height? Imagine that 20 scientists want to test this hypothesis. Each scientist, independently of the others, brings in 1,000 pairs of twins – one Taurus and one Leo – and measures the difference in their heights. Assume that in reality, the null hypothesis is true: there is no difference in their heights. Of course, there is random variation that will make each twin’s height different, but the null hypothesis is that the mean of this variation is zero.\n\nEspecially for height, it’s safe to assume that the variation in heights is normally distributed.\nEach scientists measures their 1,000 pairs of twins, conducts a t-test, and reports whether or not their statistical test has rejected the null hypothesis of no difference. If the null hypothesis is true (which we assume it is in this scenario), and the p-value threshold for significance is set at 0.05 = 1/20, we would expect that on average, 1 out of the 20 scientists would wrongly reject the null hypothesis when it is in fact true. This is the definition of type I error rate. But what if the scientists took multiple looks at the data?\nIt may have began as a way to save money – participants can be expensive! Starting with the 10th pair of twins, each scientist tests their hypothesis with a statistical test after every data point comes in. Measure the 11th pair. Run the test on the 11 data points. Measure the 12th pair. Run the test on the 12 data points. And so on. If any of these tests are significant, can the scientist claim that their result is statistically significant at the 0.05 level? Is the type I error rate of their testing procedure controlled at 0.05?\nThe answer is wholeheartedly, no. Repeated testing of the data will greatly inflate the type I error rate. Instead of the nominal 1/20 probability to wrongly reject the null hypothesis when it is true, the type I error rate of this sequential testing procedure will be much higher. If sampling continues indefinitely, sequential testing is guaranteed to reach a significant result. Edwards, Lindman, and Savage (1963) phrase this succinctly: “And indeed if an experimenter uses this procedure, then with probability 1 he will eventually reject any sharp null hypothesis, even though it be true.”\n\nEdwards, W., Lindman, H., & Savage, L. J. (1963). Bayesian statistical inference for psychological research. Psychological review, 70(3), 193.\nSimulation using R\nUsing R, let’s simulate the scenario described above and see for ourselves what happens to the type I error rate. To recap, we have 20 independent scientists conducting their own experiment. After every data point comes in, they conduct a statistical testing using the data they have collected so far. As we can see by the wiggling lines, given that the null hypothesis is true, the p-value fluctuate wildly with each new test. If a scientist obtains a p-value < 0.05 in the course of the experiment, the line past that observation is colored red so that we can identify when significance has been declared by each scientist.\n\nThe animations were created using the gganimate package.\nOn the right panel, the cumulative type I error rate and the type I error rate at each observation is shown. We can see that by the time all 1000 data points are recorded, close to half of the scientists have declared significance at some point in the process! This error rate is clearly higher than 1/20 as would be expected without sequential testing. At any given time point however, the type I error rate is preserved: only about 1 scientist will have wrongly declared significance.\nTo verify our conclusions, let’s run this simulation for 2,000 scientists instead of 20 and observe the type I error rate.\nAs seen before, the cumulative type I error rate increases the more looks the scientists take at the data, reaching almost 50% by 1,000 observations. Contrast this to type I error rate at any given observation, which remains controlled at the nominal value of 0.05.\nSo how can we control type I error?\nHow can we fix this problem? One approach is creating rules limiting the number of interim tests that can be conducted. For example, a topic of recent relevance, there was much debate over how many interim looks at the data there should be for the COVID-19 vaccine trials.\nAnother approach is applying corrections to the p-value threshold to ensure that the overall type I error rate is still 5%. The field of sequential analysis is concerned with these types of problems, and the solutions can be mathematically complex.\n\n\n\n",
    "preview": "posts/2020-12-23-early-stopping-inflates-type-i-error/preview.jpg",
    "last_modified": "2021-01-07T00:42:26-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-08-what-is-a-statistic/",
    "title": "What is a statistic?",
    "description": "Exploring the idea of a statistic by simulating dice rolls in R",
    "author": [
      {
        "name": "Maximilian Rohde",
        "url": {}
      }
    ],
    "date": "2020-12-08",
    "categories": [],
    "contents": "\n\n\nlibrary(tidyverse)\nlibrary(extrafont)\n\n\n\nIntroduction\nWhen we collect data from a data-generating process, we can calculate values from that data. These values are called statistics.\nCommon example include:\n\nmean and median (measures of center)\n\n\nvariance and IQR (measures of spread)\n\n\norder statistics, such as the minimum and the maximum\n\nWe can even create arbitrary statistics that appear to have little use, such as adding only the first and third elements of the data and dividing by 17, which we would write as \\(\\frac{x_1 + x_3}{17}\\).\nSimulating statistics of dice rolls\nAs a simple data-generating process, let’s consider rolling 5 dice. Each time we roll, we obtain 5 numbers, each from 1 to 6. We will call each one of these vectors of 5 numbers, \\[\n(x_1, x_2, x_3, x_4, x_5)\n\\] a sample. We then will compute statistics from these samples. The main question we seek to answer is: how are the statistics distributed? When I calculate the mean of 5 dice, what will the most likely result be? We can ask this question about any statistic.\nWe’ll write a function to roll n dice called roll().\n\n\n# A function to roll `n` dice\nroll <- function(n){\n  sample(x = 1:6, size=n, replace=TRUE)\n}\n\n\n\nThen we’ll use purrr:map() to generate 100,000 rolls of 5 dice.\n\n\n# Roll 5 dice 100,000 times\ndata <- map(1:1e5, ~roll(5))\n\n\n\nHere’s an example of running the function.\n\n\n# Look at first three rolls\ndata[1:3]\n\n\n[[1]]\n[1] 5 3 1 5 4\n\n[[2]]\n[1] 2 3 1 3 1\n\n[[3]]\n[1] 4 4 5 5 1\n\nFor each of these rolls, we can calculate the value of a statistic.\nWe’ll calculate the following statistics:\nmedian\nmean\nminimum\nmaximum\nsecond order statistic \\(X_{(2)}\\)\nrange\n\n\n# Returns the nth order statistic of the sample\norder_stat <- function(x, n){\n  x <- sort(x)\n  return(x[n])\n}\n\n# Generate various statistics for each roll\nmedians <- map_dbl(data, ~median(.x))\nmeans <- map_dbl(data, ~mean(.x))\nminimums <- map_dbl(data, ~min(.x))\nmaximums <- map_dbl(data, ~max(.x))\nsecond_order_stat <- map_dbl(data, ~order_stat(x=.x, n=2))\nranges <- maximums - minimums\n\n\n\n\n\n# Create a data frame from our computed statistics\ndf <- tibble(medians, means, minimums, maximums, second_order_stat, ranges)\n\n# Pivot the data into long format for plotting\ndf <- pivot_longer(df, cols = everything())\n\n\n\nNow using the data from our simulation, we can plot the sampling distribution of the each of the statistics.\n\n\ndf$name <- recode(df$name,\n  `medians` = \"Median\",\n  `means` = \"Mean\",\n  `minimums` = \"Minimum\",\n  `maximums` = \"Maximum\",\n  `second_order_stat` = \"2nd Order Statistic\",\n  `ranges` = \"Range\")\n\ndf$name <- as.factor(df$name)\ndf$name <- fct_relevel(df$name,\n                       c(\"Minimum\",\n                         \"2nd Order Statistic\",\n                         \"Maximum\",\n                         \"Range\",\n                         \"Mean\",\n                         \"Median\"))\n\ndf %>%\n  ggplot(aes(x = value)) +\n  geom_bar(aes(y = ..prop..),\n           width = 0.2, fill = \"gray\", color = \"black\") +\n  scale_x_continuous(breaks = 0:6) +\n  facet_wrap(~name, scales = \"free_x\") +\n  labs(x = \"Value\",\n       y = \"Estimated Probability\",\n       title = \"Distribution of various statistics for 100,000 rolls of 5 dice\",\n       caption = \"Monte Carlo estimate with 100,000 simulations\") +\n  ggthemes::theme_solarized() +\n  theme(text = element_text(size = 12, family = \"Source Sans Pro\"))\n\n\n\n\nFigure 1: Distributions of statistics computed from rolls of 5 dice. Probabilities were estimated using 100,000 simulations.\n\n\n\nA few things to note:\nBecause of averaging, the mean can take on more possible values than the other statistics. Qe can see it taking on the characteristic bell shape of the normal distribution due to the central limit theorem.\nThe median is always a whole number because we are rolling an odd number of dice.\nSome of these distributions are tedious to work out analytically, and with more complicated data-generating processes there may be no closed form solutions.\n\n\n\n",
    "preview": "posts/2020-12-08-what-is-a-statistic/preview.png",
    "last_modified": "2021-01-06T02:46:22-06:00",
    "input_file": {},
    "preview_width": 640,
    "preview_height": 391
  }
]
