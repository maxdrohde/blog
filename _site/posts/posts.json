[
  {
    "path": "posts/2020-12-23-early-stopping-inflates-type-i-error/",
    "title": "Repeated Testing Inflates Type I Error",
    "description": "Type I error is increased when you test your hypothesis multiple times during the data collection process. Simulations can provide a clear picture of this process.",
    "author": [
      {
        "name": "Maximilian Rohde",
        "url": "https://maximilianrohde.com"
      }
    ],
    "date": "2020-12-23",
    "categories": [],
    "contents": "\nWhat’s the problem with repeated testing?\nDoes your astrological sign affect your height? Imagine that 20 scientists want to test this hypothesis. Each scientist, independently of the others, brings in 1,000 pairs of twins – one Taurus and one Leo – and measures the difference in their heights. Assume that in reality, the null hypothesis is true: there is no difference in their heights. Of course, there is random variation that will make each twin’s height different, but the null hypothesis is that the mean of this variation is zero.\n\nEspecially for height, it’s safe to assume that the variation in heights is normally distributed.\nEach scientists measures their 1,000 pairs of twins, conducts a t-test, and reports whether or not their statistical test has rejected the null hypothesis of no difference. If the null hypothesis is true (which we assume it is in this scenario), and the p-value threshold for significance is set at 0.05 = 1/20, we would expect that on average, 1 out of the 20 scientists would wrongly reject the null hypothesis when it is in fact true. This is the definition of type I error rate. But what if the scientists took multiple looks at the data?\nIt may have began as a way to save money – participants can be expensive! Starting with the 10th pair of twins, each scientist tests their hypothesis with a statistical test after every data point comes in. Measure the 11th pair. Run the test on the 11 data points. Measure the 12th pair. Run the test on the 12 data points. And so on. If any of these tests are significant, can the scientist claim that their result is statistically significant at the 0.05 level? Is the type I error rate of their testing procedure controlled at 0.05?\nThe answer is wholeheartedly, no. Repeated testing of the data will greatly inflate the type I error rate. Instead of the nominal 1/20 probability to wrongly reject the null hypothesis when it is true, the type I error rate of this sequential testing procedure will be much higher. If sampling continues indefinitely, sequential testing is guaranteed to reach a significant result. Edwards, Lindman, and Savage (1963) phrase this succinctly: “And indeed if an experimenter uses this procedure, then with probability 1 he will eventually reject any sharp null hypothesis, even though it be true.”\n\nEdwards, W., Lindman, H., & Savage, L. J. (1963). Bayesian statistical inference for psychological research. Psychological review, 70(3), 193.\nSimulation using R\nUsing R, let’s simulate the scenario described above and see for ourselves what happens to the type I error rate. To recap, we have 20 independent scientists conducting their own experiment. After every data point comes in, they conduct a statistical testing using the data they have collected so far. As we can see by the wiggling lines, given that the null hypothesis is true, the p-value fluctuate wildly with each new test. If a scientist obtains a p-value < 0.05 in the course of the experiment, the line past that observation is colored red so that we can identify when significance has been declared by each scientist.\n\nThe animations were created using the gganimate package.\nOn the right panel, the cumulative type I error rate and the type I error rate at each observation is shown. We can see that by the time all 1000 data points are recorded, close to half of the scientists have declared significance at some point in the process! This error rate is clearly higher than 1/20 as would be expected without sequential testing. At any given time point however, the type I error rate is preserved: only about 1 scientist will have wrongly declared significance.\nTo verify our conclusions, let’s run this simulation for 2,000 scientists instead of 20 and observe the type I error rate.\nAs seen before, the cumulative type I error rate increases the more looks the scientists take at the data, reaching almost 50% by 1,000 observations. Contrast this to type I error rate at any given observation, which remains controlled at the nominal value of 0.05.\nSo how can we control type I error?\nHow can we fix this problem? One approach is creating rules limiting the number of interim tests that can be conducted. For example, a topic of recent relevance, there was much debate over how many interim looks at the data there should be for the COVID-19 vaccine trials.\nAnother approach is applying corrections to the p-value threshold to ensure that the overall type I error rate is still 5%. The field of sequential analysis is concerned with these types of problems, and the solutions can be mathematically complex.\n\n\n\n",
    "preview": "posts/2020-12-23-early-stopping-inflates-type-i-error/preview.jpg",
    "last_modified": "2020-12-30T02:40:03-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-08-what-is-a-statistic/",
    "title": "What is a Statistic?",
    "description": "An introduction to the definition of \"statistic\"",
    "author": [
      {
        "name": "Maximilian Rohde",
        "url": {}
      }
    ],
    "date": "2020-12-08",
    "categories": [],
    "contents": "\n\n\nlibrary(tidyverse)\nlibrary(extrafont)\n\n\n\nIntroduction\nWhen we collect data from a data-generating process, we can calculate values from that data. These values are called statistics.\nCommon example include:\n\nmean and median (measures of center)\n\n\nvariance and IQR (measures of spread)\n\n\norder statistics, such as the minimum and the maximum\n\nWe can even create arbitrary statistics that appear to have little use, such as adding only the first and third elements of the data and dividing by 17, which we would write as \\(\\frac{x_1 + x_3}{17}\\).\nSimulating statistics of dice rolls\nAs a simple data-generating process, let’s consider rolling 5 dice. Each time we roll, we obtain 5 numbers, each from 1 to 6. We will call each one of these vectors of 5 numbers, \\[\n(x_1, x_2, x_3, x_4, x_5)\n\\] a sample. We then will compute statistics from these samples. The main question we seek to answer is: how are the statistics distributed? When I calculate the mean of 5 dice, what will the most likely result be? We can ask this question about any statistic.\nWe’ll write a function to roll n dice called roll().\n\n\n# A function to roll `n` dice\nroll <- function(n){\n  sample(x = 1:6, size=n, replace=TRUE)\n}\n\n\n\nThen we’ll use purrr:map() to generate 100,000 rolls of 5 dice.\n\n\n# Roll 5 dice 100,000 times\ndata <- map(1:1e5, ~roll(5))\n\n\n\nHere’s an example of running the function.\n\n\n# Look at first three rolls\ndata[1:3]\n\n\n[[1]]\n[1] 6 6 4 6 5\n\n[[2]]\n[1] 6 3 1 6 4\n\n[[3]]\n[1] 1 1 1 3 3\n\nFor each of these rolls, we can calculate the value of a statistic.\nWe’ll calculate the following statistics:\nmedian\nmean\nminimum\nmaximum\nsecond order statistic \\(X_{(2)}\\)\nrange\n\n\n# Returns the nth order statistic of the sample\norder_stat <- function(x, n){\n  x <- sort(x)\n  return(x[n])\n}\n\n# Generate various statistics for each roll\nmedians <- map_dbl(data, ~median(.x))\nmeans <- map_dbl(data, ~mean(.x))\nminimums <- map_dbl(data, ~min(.x))\nmaximums <- map_dbl(data, ~max(.x))\nsecond_order_stat <- map_dbl(data, ~order_stat(x=.x, n=2))\nranges <- maximums - minimums\n\n\n\n\n\n# Create a data frame from our computed statistics\ndf <- tibble(medians, means, minimums, maximums, second_order_stat, ranges)\n\n# Pivot the data into long format for plotting\ndf <- pivot_longer(df, cols = everything())\n\n\n\nNow using the data from our simulation, we can plot the sampling distribution of the each of the statistics.\n\n\ndf$name <- recode(df$name,\n  `medians` = \"Median\",\n  `means` = \"Mean\",\n  `minimums` = \"Minimum\",\n  `maximums` = \"Maximum\",\n  `second_order_stat` = \"2nd Order Statistic\",\n  `ranges` = \"Range\")\n\ndf$name <- as.factor(df$name)\ndf$name <- fct_relevel(df$name,\n                       c(\"Minimum\",\n                         \"2nd Order Statistic\",\n                         \"Maximum\",\n                         \"Range\",\n                         \"Mean\",\n                         \"Median\"))\n\ndf %>%\n  ggplot(aes(x = value)) +\n  geom_bar(aes(y = ..prop..),\n           width = 0.2, fill = \"gray\", color = \"black\") +\n  scale_x_continuous(breaks = 0:6) +\n  facet_wrap(~name, scales = \"free_x\") +\n  labs(x = \"Value\",\n       y = \"Estimated Probability\",\n       title = \"Distribution of various statistics for 100,000 rolls of 5 dice\",\n       caption = \"Monte Carlo estimate with 100,000 simulations\") +\n  ggthemes::theme_solarized() +\n  theme(text = element_text(size = 12, family = \"Source Sans Pro\"))\n\n\n\n\nFigure 1: Distributions of statistics computed from rolls of 5 dice. Probabilities were estimated using 100,000 simulations.\n\n\n\nA few things to note:\nBecause of averaging, the mean can take on more possible values than the other statistics. Qe can see it taking on the characteristic bell shape of the normal distribution due to the central limit theorem.\nThe median is always a whole number because we are rolling an odd number of dice.\nSome of these distributions are tedious to work out analytically, and with more complicated data-generating processes there may be no closed form solutions.\n\n\n\n",
    "preview": "posts/2020-12-08-what-is-a-statistic/preview.png",
    "last_modified": "2020-12-30T02:42:35-06:00",
    "input_file": {},
    "preview_width": 1402,
    "preview_height": 856
  }
]
