[
  {
    "path": "posts/2021-01-06-gganimatemap/",
    "title": "Using gganimate with ggmap",
    "description": "Combining gganimate with ggmap can be used to create animations of geographic data. I give a few examples using data from Nashville Open Data.",
    "author": [
      {
        "name": "Maximilian Rohde",
        "url": "https://maximilianrohde.com"
      }
    ],
    "date": "2021-01-06",
    "categories": [],
    "contents": "\n\n\nlibrary(tidyverse)\nlibrary(ggmap)\nlibrary(gganimate)\n\n# ggplot2 themes\nlibrary(cowplot)\n\n# Formatting of HTML tables\nlibrary(kableExtra)\n\n\n\nOverview\nAnimating your ggplot2 visualizations is easy using the gganimate package. But did you also know that gganimate can be used with the ggmap package to animate geographic data? Using data from Nashville Open Data, we’ll create an animation to visualize the spatial evolution of parks in Nashville over time.\nData cleaning and exploration\nFirst, let’s load in the data. We can load the data directly from the URL using read_csv().\n\n\n# Read in data\ndf <- read_csv(\"https://data.nashville.gov/resource/74d7-b74t.csv\")\n\n\n\nLet’s take a look at the first couple rows.\n\n\nhead(df, 2) %>% \n  # table styling for HTML output\n  kable(format = \"html\") %>%\n  kable_styling(\"striped\") %>%\n  scroll_box(width = \"100%\")\n\n\n\n\npark_name\n\n\nacres\n\n\nyear_established\n\n\ncommunity_center\n\n\nnature_center\n\n\nplayground\n\n\nada_accessible\n\n\nrestrooms_available\n\n\npicnic_shelters\n\n\npicnic_shelters_quantity\n\n\ndog_park\n\n\nbaseball_fields\n\n\nbasketball_courts\n\n\nvolleyball\n\n\nsoccer_fields\n\n\nfootball_multi_purpose_fields\n\n\ntennis_courts\n\n\ndisc_golf\n\n\nskate_park\n\n\nswimming_pool\n\n\nspray_park\n\n\ngolf_course\n\n\nwalk_jog_paths\n\n\nhiking_trails\n\n\nhorse_trails\n\n\nmountain_bike_trails\n\n\nboat_launch\n\n\ncamping_available_by_permit\n\n\nfishing_by_permit\n\n\nlake\n\n\ncanoe_launch\n\n\ncommunity_garden\n\n\nhistoric_features\n\n\nnotes\n\n\nmapped_location\n\n\nParadise Ridge Park\n\n\n98.41\n\n\n2013\n\n\nYes\n\n\nNo\n\n\nYes\n\n\nYes\n\n\nYes\n\n\nYes\n\n\n1\n\n\nNo\n\n\nNo\n\n\nYes\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nYes\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\ncommunity meeting space, gym, playground, walking trail, 1 reservable picnic shelter, outdoor basketball court\n\n\n3000 Morgan Rd Joelton, TN 37080 (36.335583, -86.85995)\n\n\nCrooked Branch Park\n\n\n84.00\n\n\n2012\n\n\nNA\n\n\nNA\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\n0\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNatural area with walking trails\n\n\n116D Ray Ave Lakewood, TN 37138 (36.241016, -86.640834)\n\n\n\nA few basic questions I have about the dataset are the following:\nAre parks duplicated?\n\n\ndf %>%\n  filter(duplicated(park_name)) %>%\n  kable(format = \"html\") %>%\n  kable_styling(\"striped\") %>%\n  scroll_box(width = \"100%\")\n\n\n\n\npark_name\n\n\nacres\n\n\nyear_established\n\n\ncommunity_center\n\n\nnature_center\n\n\nplayground\n\n\nada_accessible\n\n\nrestrooms_available\n\n\npicnic_shelters\n\n\npicnic_shelters_quantity\n\n\ndog_park\n\n\nbaseball_fields\n\n\nbasketball_courts\n\n\nvolleyball\n\n\nsoccer_fields\n\n\nfootball_multi_purpose_fields\n\n\ntennis_courts\n\n\ndisc_golf\n\n\nskate_park\n\n\nswimming_pool\n\n\nspray_park\n\n\ngolf_course\n\n\nwalk_jog_paths\n\n\nhiking_trails\n\n\nhorse_trails\n\n\nmountain_bike_trails\n\n\nboat_launch\n\n\ncamping_available_by_permit\n\n\nfishing_by_permit\n\n\nlake\n\n\ncanoe_launch\n\n\ncommunity_garden\n\n\nhistoric_features\n\n\nnotes\n\n\nmapped_location\n\n\nRiverfront Park\n\n\n5.29\n\n\n1983\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nYes\n\n\nNo\n\n\n0\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nYes\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nYes\n\n\nNo\n\n\nAmphitheater, Pleasure/Commercial Boat Docking\n\n\n100 1st Ave South Nashville, TN (36.162279, -86.774364)\n\n\nRiverfront Park\n\n\n12.54\n\n\n2015\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nYes\n\n\nNo\n\n\nNo\n\n\n0\n\n\nYes\n\n\nNo\n\n\nYes\n\n\nNo\n\n\nNo\n\n\nYes\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nYes\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nOutdoor fitness equipment, dog park, Ascend Amphitheater\n\n\n310 1st Ave S Nashville, TN (36.159082, -86.772376)\n\n\nRiverfront Park\n\n\n3.49\n\n\n1977\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\n0\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nJames Robertson Statue, ,small stair amphitheater\n\n\n170 1st Ave N Nashville, TN (36.164111, -86.77555)\n\n\n\nIt looks like three parks have the name Riverfront Park, but judging by year_established and mapped_location, they are different parks.\nHow many parks are there?\n\n\nnrow(df)\n\n\n[1] 123\n\n123 parks in Nashville – not bad!\nWhich years are represented in the data?\n\n\nrange(df$year_established)\n\n\n[1]    0 2015\n\nStrange, it looks like the oldest park was established in year “0”. This must be a mistake.\n\n\n# Newest 5 parks\nsort(df$year_established) %>% tail()\n\n\n[1] 2013 2013 2014 2014 2015 2015\n\n# Oldest 5 parks\nsort(df$year_established) %>% head()\n\n\n[1]    0 1901 1903 1907 1909 1910\n\nThe true range of the data is 1901 - 2015. It looks like the park recorded as being established in year zero was a mistake. Let’s remove it from the dataset.\n\n\ndf <-\n  df %>%\n  filter(year_established != 0)\n\n\n\nNow let’s take a look at the distribution of the years when parks were established. First, we’ll make a histogram.\n\n\ndf %>%\n  ggplot() +\n  aes(year_established) +\n  geom_histogram(bins=30, color=\"black\", fill=\"grey\") +\n  labs(\n    title = \"Number of parks established in Nashville per year\",\n    subtitle = \"\",\n    x= \"Year Established\",\n    y= \"Frequency\") +\n  cowplot::theme_cowplot(font_family = \"Source Sans Pro\",\n                         font_size = 12)\n\n\n\n\nThe rate of new park development looks to be increasing over time. An ECDF plot supports this observation.\n\n\ndf %>%\n  ggplot()+\n  aes(year_established) +\n  stat_ecdf() +\n  labs(\n    title = \"Cumulative distribution of Nashville parks over time\",\n    subtitle = \"\",\n    x= \"Year Established\",\n    y= \"ECDF\") +\n  cowplot::theme_cowplot(font_family = \"Source Sans Pro\",\n                         font_size = 12)\n\n\n\n\nCreating the Animation\nNow that we’ve cleaned and explored the data, let’s create an animation with gganimate and ggmap. I would like to visualize the spatial evolution of Nashville’s parks by year.\nFirst, let’s try to just plot the locations of all the parks. We see that location is stored in the mapped_location column.\n\n\nhead(df$mapped_location)\n\n\n[1] \"3000 Morgan Rd\\nJoelton, TN 37080\\n(36.335583, -86.85995)\"                 \n[2] \"116D Ray Ave\\nLakewood, TN 37138\\n(36.241016, -86.640834)\"                 \n[3] \"21 Joelton Community Center Rd\\nJoelton, TN 37080\\n(36.316932, -86.870111)\"\n[4] \"1266 Stones River Road\\nHermitage, TN 37076\\n(36.189647, -86.652059)\"      \n[5] \"Grand Ave at 14th Ave\\nNashville, TN 37212\\n(36.1461984, -86.789001)\"      \n[6] \"Snell Blvd at Panorama Dr\\nNashville, TN \\n(36.1797136, -86.8395478)\"      \n\nLuckily, the (latitude, longitude) coordinates are provided, but we need to extract them from the text. We can use a regular expression to do this using the stringr package.\n\n\n# Get a matrix of matched latitudes and longitudes\nmatch <- str_match(df$mapped_location, \"\\\\((.*), (.*)\\\\)\")\n\n# Add the latitudes and longitudes to the data frame\n# data is recorded as character, so need to convert to numeric\ndf$lat <- match[,2] %>% as.numeric()\ndf$long <- match[,3] %>% as.numeric()\n\n\n\nLet’s verify we extracted the coordinates correctly.\n\n\n# View first 5 coordinates\ndf %>%\n  select(lat, long) %>%\n  head() %>%\n  kable(format = \"html\") %>%\n  kable_styling(\"striped\") %>%\n  scroll_box(width = \"100%\")\n\n\n\n\nlat\n\n\nlong\n\n\n36.33558\n\n\n-86.85995\n\n\n36.24102\n\n\n-86.64083\n\n\n36.31693\n\n\n-86.87011\n\n\n36.18965\n\n\n-86.65206\n\n\n36.14620\n\n\n-86.78900\n\n\n36.17971\n\n\n-86.83955\n\n\n\nNow we can make the plot. We start with the qmplot() function from ggmap, which is a shortcut for plotting on maps, just like qplot() in ggplot2. We pass in the latitude and longitude coordinates and the data frame. The argument maptype = \"toner-lite\" indicates the type of basemap to use as the background. We also specify alpha=0.5 so we can see when the points overlap. I would like larger parks to be represented by larger circles, so we can map size to acreage by aes(size=acres). Then we add the extra theming using the cowplot package.\n\n\nqmplot(long, lat, data = df,\n       maptype = \"toner-lite\", alpha=0.5) + \n  aes(size=acres) +\n  labs(\n    title = \"Nashville Parks\",\n    x= \"Longitude\",\n    y= \"Latitude\",\n    caption = \"Area corresponds to acreage \\n Data available from Nashville Open Data\") +\n  cowplot::theme_cowplot(font_family = \"Source Sans Pro\",\n                         font_size = 12) +\n  theme(legend.position = \"none\")\n\n\n\n\nLooks pretty good already. Now let’s make this animated!\nWe will add the transition_states() function from gganimate and specify that each state of the animation is determined by year_established. We also set subtitle = \"Year: {closest_state}\" to display the year of the current frame.\n\n\nqmplot(long, lat, data = df,\n       maptype = \"toner-lite\", alpha=0.5) + \n  aes(size=acres, group=year_established) +\n  labs(\n    title = \"Nashville Parks\",\n    subtitle = \"Year: {closest_state}\",\n    x= \"Longitude\",\n    y= \"Latitude\",\n    caption = \"Area corresponds to acreage \\n Data available from Nashville Open Data\") +\n  cowplot::theme_cowplot(font_family = \"Source Sans Pro\",\n                         font_size = 12) +\n  theme(legend.position = \"none\") +\n  transition_states(year_established)\n\n\n\n\nIt’s animated now, but there are two problems.\nFirst, the points are disappearing after each year. We can add shadow_mark(color=\"black\") to have the points stay on the plot after they are introduced. We specify that the old points are colored black so that we can color the current points red to highlight which points were just displayed.\nSecond, the passage of time is not constant. We want to have each frame change in increments of one year. In our current animation, the years are skipping between the years present in the data. To fix this, we will to convert year_established to a factor, and fill in the missing years.\n\n\ndf$year_established <-\n  df$year_established %>%\n  # convert to factor\n  as.factor() %>%\n  # add extra years\n  fct_expand(1900:2019 %>% as.character) %>%\n  # sort years\n  fct_relevel(1900:2019 %>% as.character)\n\n\n\nNow that we’ve made those changes, let’s try again.\n\n\nqmplot(long, lat, data = df,\n       maptype = \"toner-lite\", alpha=0.5, color=\"red\") + \n  aes(size=acres, group=year_established) +\n  labs(\n    title = \"Nashville Parks\",\n    subtitle = \"Year: {closest_state}\",\n    x= \"Longitude\",\n    y= \"Latitude\",\n    caption = \"Area corresponds to acreage \\n Data available from Nashville Open Data\") +\n  cowplot::theme_cowplot(font_family = \"Source Sans Pro\",\n                         font_size = 12) +\n  theme(legend.position = \"none\") +\n  transition_states(year_established) +\n  shadow_mark(color=\"black\")\n\n\n\n\nLooks good! But wait a second… the animation only goes to 1950. Wasn’t it supposed to go to 2015? This is a little quirk of gganimate. By default, the animation is capped at 100 frames. For the transition_states() animation, by default a single frame is allocated for each year, and another frame is allocated for transitions between frames. 100 frames therefore can represent 50 years of data. The animation is cut short because we have more than 50 years of data.\nLet’s fix this by saving the animation to a variable, and then using the animate() function to increase the number of frames.\n\n\nparks_anim <- \n  qmplot(long, lat, data = df,\n       maptype = \"toner-lite\", alpha=0.5, color=\"red\") + \n  aes(size=acres, group=year_established) +\n  labs(\n    title = \"Nashville Parks\",\n    subtitle = \"Year: {closest_state}\",\n    x= \"Longitude\",\n    y= \"Latitude\",\n    caption = \"Area corresponds to acreage \\n Data available from Nashville Open Data\") +\n  cowplot::theme_cowplot(font_family = \"Source Sans Pro\",\n                         font_size = 12) +\n  theme(legend.position = \"none\") +\n  transition_states(year_established) +\n  shadow_mark(color=\"black\")\n\nanimate(\n  parks_anim,\n  nframes=300, # number of frames to compute\n  )\n\n\n\n\nMy preferred way of rendering is to use ffmpeg, instead of the default GIF rendered, because it creates videos rather than GIFs. You will need to install ffpmeg on your computer separately. Using ffmpeg also allows for finer control over the frame rate of the animation and creates smaller files. I’ll show how to use it below.\nThe animate() function has parameters for duration (total duration in seconds), fps (frames per second), and nframes (total number of frames). You can specify any two. For our case, we give the duration and number of frames, and gganimate figures out the proper frame rate to fit the specified number of frames into the specified number of seconds.\nWe also set res=300 to increase the resolution. This has the side effect of making the font appear large, so we lower the font size in the call the theme_cowplot().\nBe warned that this may take a bit of time to animate. Usually, I put my code in a .R file and run it from the terminal, rather than in RStudio, which seems to crash less and render more quickly. Otherwise, when I render the animations in an R Markdown file, I set cache=TRUE in the cell so that I don’t have to render each time I knit the document.\n\n\nparks_anim <- \n  qmplot(long, lat, data = df,\n       maptype = \"toner-lite\", alpha=0.5, color=\"red\") + \n  aes(size=acres, group=year_established) +\n  labs(\n    title = \"Nashville Parks\",\n    subtitle = \"Year: {closest_state}\",\n    x= \"Longitude\",\n    y= \"Latitude\",\n    caption = \"Area corresponds to acreage \\n Data available from Nashville Open Data\") +\n  cowplot::theme_cowplot(font_family = \"Source Sans Pro\",\n                         font_size = 10) +\n  theme(legend.position = \"none\") +\n  transition_states(year_established) +\n  shadow_mark(color=\"black\")\n\nanimate(\n  parks_anim,\n  duration=15, # duration of the animation in seconds\n  nframes=768, # number of frames to compute\n  height = 6,\n  width = 6,\n  units = \"in\",\n  res = 300, # resolution of the output\n  renderer = ffmpeg_renderer() # render to video with ffmpeg\n  )\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-01-06-gganimatemap/preview.jpg",
    "last_modified": "2021-01-07T01:31:26-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-23-early-stopping-inflates-type-i-error/",
    "title": "Repeated testing inflates type I error",
    "description": "Type I error is increased when you test your hypothesis multiple times during the data collection process. Simulations can provide a clear picture of this process.",
    "author": [
      {
        "name": "Maximilian Rohde",
        "url": "https://maximilianrohde.com"
      }
    ],
    "date": "2020-12-23",
    "categories": [],
    "contents": "\nWhat’s the problem with repeated testing?\nDoes your astrological sign affect your height? Imagine that 20 scientists want to test this hypothesis. Each scientist, independently of the others, brings in 1,000 pairs of twins – one Taurus and one Leo – and measures the difference in their heights. Assume that in reality, the null hypothesis is true: there is no difference in their heights. Of course, there is random variation that will make each twin’s height different, but the null hypothesis is that the mean of this variation is zero.\n\nEspecially for height, it’s safe to assume that the variation in heights is normally distributed.\nEach scientists measures their 1,000 pairs of twins, conducts a t-test, and reports whether or not their statistical test has rejected the null hypothesis of no difference. If the null hypothesis is true (which we assume it is in this scenario), and the p-value threshold for significance is set at 0.05 = 1/20, we would expect that on average, 1 out of the 20 scientists would wrongly reject the null hypothesis when it is in fact true. This is the definition of type I error rate. But what if the scientists took multiple looks at the data?\nIt may have began as a way to save money – participants can be expensive! Starting with the 10th pair of twins, each scientist tests their hypothesis with a statistical test after every data point comes in. Measure the 11th pair. Run the test on the 11 data points. Measure the 12th pair. Run the test on the 12 data points. And so on. If any of these tests are significant, can the scientist claim that their result is statistically significant at the 0.05 level? Is the type I error rate of their testing procedure controlled at 0.05?\nThe answer is wholeheartedly, no. Repeated testing of the data will greatly inflate the type I error rate. Instead of the nominal 1/20 probability to wrongly reject the null hypothesis when it is true, the type I error rate of this sequential testing procedure will be much higher. If sampling continues indefinitely, sequential testing is guaranteed to reach a significant result. Edwards, Lindman, and Savage (1963) phrase this succinctly: “And indeed if an experimenter uses this procedure, then with probability 1 he will eventually reject any sharp null hypothesis, even though it be true.”\n\nEdwards, W., Lindman, H., & Savage, L. J. (1963). Bayesian statistical inference for psychological research. Psychological review, 70(3), 193.\nSimulation using R\nUsing R, let’s simulate the scenario described above and see for ourselves what happens to the type I error rate. To recap, we have 20 independent scientists conducting their own experiment. After every data point comes in, they conduct a statistical testing using the data they have collected so far. As we can see by the wiggling lines, given that the null hypothesis is true, the p-value fluctuate wildly with each new test. If a scientist obtains a p-value < 0.05 in the course of the experiment, the line past that observation is colored red so that we can identify when significance has been declared by each scientist.\n\nThe animations were created using the gganimate package.\nOn the right panel, the cumulative type I error rate and the type I error rate at each observation is shown. We can see that by the time all 1000 data points are recorded, close to half of the scientists have declared significance at some point in the process! This error rate is clearly higher than 1/20 as would be expected without sequential testing. At any given time point however, the type I error rate is preserved: only about 1 scientist will have wrongly declared significance.\nTo verify our conclusions, let’s run this simulation for 2,000 scientists instead of 20 and observe the type I error rate.\nAs seen before, the cumulative type I error rate increases the more looks the scientists take at the data, reaching almost 50% by 1,000 observations. Contrast this to type I error rate at any given observation, which remains controlled at the nominal value of 0.05.\nSo how can we control type I error?\nHow can we fix this problem? One approach is creating rules limiting the number of interim tests that can be conducted. For example, a topic of recent relevance, there was much debate over how many interim looks at the data there should be for the COVID-19 vaccine trials.\nAnother approach is applying corrections to the p-value threshold to ensure that the overall type I error rate is still 5%. The field of sequential analysis is concerned with these types of problems, and the solutions can be mathematically complex.\n\n\n\n",
    "preview": "posts/2020-12-23-early-stopping-inflates-type-i-error/preview.jpg",
    "last_modified": "2021-01-07T00:42:26-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-08-what-is-a-statistic/",
    "title": "What is a statistic?",
    "description": "Exploring the idea of a statistic by simulating dice rolls in R",
    "author": [
      {
        "name": "Maximilian Rohde",
        "url": {}
      }
    ],
    "date": "2020-12-08",
    "categories": [],
    "contents": "\n\n\nlibrary(tidyverse)\nlibrary(extrafont)\n\n\n\nIntroduction\nWhen we collect data from a data-generating process, we can calculate values from that data. These values are called statistics.\nCommon example include:\n\nmean and median (measures of center)\n\n\nvariance and IQR (measures of spread)\n\n\norder statistics, such as the minimum and the maximum\n\nWe can even create arbitrary statistics that appear to have little use, such as adding only the first and third elements of the data and dividing by 17, which we would write as \\(\\frac{x_1 + x_3}{17}\\).\nSimulating statistics of dice rolls\nAs a simple data-generating process, let’s consider rolling 5 dice. Each time we roll, we obtain 5 numbers, each from 1 to 6. We will call each one of these vectors of 5 numbers, \\[\n(x_1, x_2, x_3, x_4, x_5)\n\\] a sample. We then will compute statistics from these samples. The main question we seek to answer is: how are the statistics distributed? When I calculate the mean of 5 dice, what will the most likely result be? We can ask this question about any statistic.\nWe’ll write a function to roll n dice called roll().\n\n\n# A function to roll `n` dice\nroll <- function(n){\n  sample(x = 1:6, size=n, replace=TRUE)\n}\n\n\n\nThen we’ll use purrr:map() to generate 100,000 rolls of 5 dice.\n\n\n# Roll 5 dice 100,000 times\ndata <- map(1:1e5, ~roll(5))\n\n\n\nHere’s an example of running the function.\n\n\n# Look at first three rolls\ndata[1:3]\n\n\n[[1]]\n[1] 5 3 1 5 4\n\n[[2]]\n[1] 2 3 1 3 1\n\n[[3]]\n[1] 4 4 5 5 1\n\nFor each of these rolls, we can calculate the value of a statistic.\nWe’ll calculate the following statistics:\nmedian\nmean\nminimum\nmaximum\nsecond order statistic \\(X_{(2)}\\)\nrange\n\n\n# Returns the nth order statistic of the sample\norder_stat <- function(x, n){\n  x <- sort(x)\n  return(x[n])\n}\n\n# Generate various statistics for each roll\nmedians <- map_dbl(data, ~median(.x))\nmeans <- map_dbl(data, ~mean(.x))\nminimums <- map_dbl(data, ~min(.x))\nmaximums <- map_dbl(data, ~max(.x))\nsecond_order_stat <- map_dbl(data, ~order_stat(x=.x, n=2))\nranges <- maximums - minimums\n\n\n\n\n\n# Create a data frame from our computed statistics\ndf <- tibble(medians, means, minimums, maximums, second_order_stat, ranges)\n\n# Pivot the data into long format for plotting\ndf <- pivot_longer(df, cols = everything())\n\n\n\nNow using the data from our simulation, we can plot the sampling distribution of the each of the statistics.\n\n\ndf$name <- recode(df$name,\n  `medians` = \"Median\",\n  `means` = \"Mean\",\n  `minimums` = \"Minimum\",\n  `maximums` = \"Maximum\",\n  `second_order_stat` = \"2nd Order Statistic\",\n  `ranges` = \"Range\")\n\ndf$name <- as.factor(df$name)\ndf$name <- fct_relevel(df$name,\n                       c(\"Minimum\",\n                         \"2nd Order Statistic\",\n                         \"Maximum\",\n                         \"Range\",\n                         \"Mean\",\n                         \"Median\"))\n\ndf %>%\n  ggplot(aes(x = value)) +\n  geom_bar(aes(y = ..prop..),\n           width = 0.2, fill = \"gray\", color = \"black\") +\n  scale_x_continuous(breaks = 0:6) +\n  facet_wrap(~name, scales = \"free_x\") +\n  labs(x = \"Value\",\n       y = \"Estimated Probability\",\n       title = \"Distribution of various statistics for 100,000 rolls of 5 dice\",\n       caption = \"Monte Carlo estimate with 100,000 simulations\") +\n  ggthemes::theme_solarized() +\n  theme(text = element_text(size = 12, family = \"Source Sans Pro\"))\n\n\n\n\nFigure 1: Distributions of statistics computed from rolls of 5 dice. Probabilities were estimated using 100,000 simulations.\n\n\n\nA few things to note:\nBecause of averaging, the mean can take on more possible values than the other statistics. Qe can see it taking on the characteristic bell shape of the normal distribution due to the central limit theorem.\nThe median is always a whole number because we are rolling an odd number of dice.\nSome of these distributions are tedious to work out analytically, and with more complicated data-generating processes there may be no closed form solutions.\n\n\n\n",
    "preview": "posts/2020-12-08-what-is-a-statistic/preview.png",
    "last_modified": "2021-01-06T02:46:22-06:00",
    "input_file": {},
    "preview_width": 1402,
    "preview_height": 856
  }
]
