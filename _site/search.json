{
  "articles": [
    {
      "path": "about.html",
      "title": "⠀",
      "description": "",
      "author": [],
      "contents": "\n\n\n\n\nHello, I’m Max. I have a BA in Physics and Geology from Carleton College\nand I’m currently a PhD student at Vanderbilt University in the Department\nof Biostatistics. Currently I’m conducting research on Bayesian\nclinical trials designs for COVID-19 with Dr. Frank Harrell, and I’m a\nstatistician on the ACTIV-6 trial.\n\n\n\nAbout me\nI worked as an ORISE Fellow from 2018-2020 in the FDA\nDivision of Antivirals (Office of New Drugs), where I built\nnatural-language processing tools for product label data, and conducted\nmeta-analyses of clinical trials for hepatitis C and HIV\ntreatments. My interests are in adaptive clinical trial design,\nreproducible research, and statistics education.\nIn addition to my work at the FDA, I have conducted research with the\nLIGO collaboration on the effects of magnetic transients on\ngravitational wave detection, studied the effects of atmospheric noise on\nseismometers at the Albuquerque Seismological Laboratory, and worked\nin science policy at the MIT Washington\nOffice in Washington, DC.\nCommunicating science is as important as research to me. The past few\nyears I have been working as a tutor, where I love teaching\nphysics, chemistry, and mathematics to high-school and undergraduate\nstudents\nIn my free time I enjoy cooking (and collecting cookbooks), playing\nfunk guitar, and swimming.\nContact\nYou can contact me at\nmaxdrohde@gmail.com. I also\npost on Twitter.\nMy GitHub can here found here. I mainly code in R and\nPython, and I’m starting to learn to code in Julia.\nYou can find my publications at my Google Scholar page\nhere.\n\n\n\n",
      "last_modified": "2022-03-31T10:53:32-05:00"
    },
    {
      "path": "anim.html",
      "title": "Animations",
      "description": "",
      "author": [],
      "contents": "\nBelow are some animations I created using gganimate to help me\nunderstand concepts in statistics. The work of 3Blue1Brown and Seeing Theory have been an\ninspiration for me.\nOverfitting with Regression\nSplines\nRegression splines are an effective tool for fitting curves to\nnon-linear data. However, the flexibility of splines is both a blessing\nand a curse. If the degrees of freedom are set too high, splines can\nquickly overfit a dataset – resulting in good performance on the\ntraining data but poor performance on the testing data.\nThe below animation shows training data (top left) and test data (top\nright), with the true data generating function as the gray dashed line.\nWe see that the best performance on the test set is obtained at about 5\ndegrees of freedom, while the training error goes to zero, a clear sign\nof overfitting.\nIllustrating\nthe bias-variance tradeoff with the KNN and Least-Squares\nclassifiers\nComparing the least-squares classifier to the KNN classifier, fit on\nbootstrap resamples, demonstrates the bias-variance tradeoff. The black\nline is the true 0.5 probability contour, the red line is the estimated\n0.5 probability contour. Data from Elements of Statistical Learning.\nExample\nof link function misspecification in GLM for binary outcomes\nBayesian Inference\nfor a Binomial Proportion\nBayesian inference can be using to quantity our uncertainty about the\nparameter \\(p\\) from a statistical\nmodel where the data is distributed \\(\\text{Bin}(n,p)\\). An example is the bias\nof a coin. In the below example, the true bias is \\(p = 0.7\\). As the data come in, we can\nupdate our posterior distribution for \\(p\\).\nk-Nearest Neighbors\nDecision Boundary\nk-Nearest Neighbors is a non-parametric classification algorithm that\nclassifies each point to the majority class of the \\(k\\) nearest points. The number of neighbors\nto use greatly affects the decision boundaries, as shown in the below\nanimation.\nSimple MCMC Animation\nBayesian Linear Regression\nBayesian inference can also be used in linear regression. As the data\nis collected, we become more confident about the parameters of the\nlinear regression model:\nthe slope of the linear relationship\nthe variance of the error term\nthe intercept of the linear relationship\nThe model was fit using Stan to do\nthe MCMC sampling and the posterior distributions were plotted using a\nkernel\ndensity estimate.\nCalculating π with\nMonte Carlo Estimation\nMonte Carlo estimation is a technique for solving deterministic\nproblems by random sampling.\nFor example, you can compute π by uniform sampling within a square\nfor \\(x \\in [-1,1], y \\in [-1,1]\\), and\nrejecting the points where \\(x^2 + y^2 >\n1\\).\nThen \\(\\pi = \\text{(Proportion not\nrejected)} \\times 4\\).\nOverfitting in Polynomial\nRegression\nOverfitting is a major problem when fitting complex models to few\ndata points. As a simple example, polynomial regression can fit noise in\nthe data rather than the true model. Here the true model is quadratic\nwith error that is normally distributed with mean zero. As the degree of\nthe polynomial increases, the model rapidly overfits the data. At the\nextreme, if the degree of the polynomial is greater than (Number of\npoints - 1), then the fitted polynomial will pass through every data\npoint.\nFinding\nthe MLE estimate of the mean for a Normal Distribution\nYou can estimate the MLE for the mean of a normal distribution given\na dataset by varying the mean until the peak of the log-likelihood is\nreached. This visually looks like sliding around different candidate\ndistributions until the best match is found. Once the MLE for the mean\nis found, the variance can be varied in the same fashion (keeping the\nmean fixed at the MLE) to find the MLE for the variance.\nExample of collider bias\n\nUsing\nQuantile-Quantile Plots to Detect Depatures from Normality\nQuantile-quantile plots are a useful tool for assessing the fit of\ndata to a given distribution. This animation shows Q-Q plots for\nt-distributed data with various degrees of freedom. We see that the Q-Q\nplot shows more clearly the departure from normality compared to the\nhistogram.\nThe German Tank Problem\nThe German Tank\nproblem is a famous problem in statistics. During World War 2, the\nAllied forces used the serial numbers on German tanks to estimate the\nnumber of tanks produced. The results of this statistical analysis\nestimated a production of 246 tanks per month, while intelligence from\nspies estimated a much higher rate of around 1.400 tanks per month.\nAfter the war, German records showed that the true rate was 245 per\nmonth! The statistical evidence that the numbers of tanks was lower than\nexpected gave the Allies motivation to attack the Western Front, leading\nto the fall of Berlin and the end of the war in Europe.\nWe can formulate this problem assuming the serial numbers start at 1\nand are randomly sampled from the population of tanks. Let \\(X_1, X_2, \\ldots X_n\\) be a sample of \\(n\\) serial numbers. The maximum likelihood\nestimate for the total number of serial numbers is \\(X_{(n)}\\), the maximum of the observed\nserial numbers. However, this is a biased estimator – it underestimates\nthe true number of tanks.\nWe can improve this estimator by adding a factor to our maximum\nlikelihood estimator to make it unbiased – the number of known missing\ntank serial numbers divided by the sample size, which can be thought of\nas the average gap between the recorded serial numbers. Thus the new\nestimator is \\[\n\\hat{\\theta}=x_{(n)}+\\frac{x_{(n)}-n}{n} = \\left(1+\\frac{1}{n}\\right)\nx_{(n)}-1\n\\] which has the desired property of being unbiased. Simulating\nthe performance of our estimators with a known population size of 2,000\ntanks, we see that bias-corrected MLE has higher variance than the MLE\nbut it achieves a lower mean squared error across the tested sample\nsizes due to being unbiased.\n\nThere are \\(X_{(n)} - n\\) known missing\ntank serial numbers.\n\nMy analysis follows that given in Leemis, L. (2020). Mathematical\nStatistics.\nComparing Estimators for\nUnif(0, θ)\nThe maximum likelihood estimator (MLE), while it has many nice\nstatistics properties, isn’t always the best estimator. To estimate θ\nfor Unif(0,θ), the MLE is the maximum of the data, which is biased low.\nIt is an underestimate of the true value of θ. To correct the bias, we\ncan multiply the maximum by \\(\\frac{n+1}{n}\\) for an estimator that is\nunbiased, but has slightly higher variance. However, we see that the\nresult is lower mean-squared error (MSE). We also show the method of\nmoments estimator for reference, which is twice the sample mean.\nComparing Estimators\nfor Center of Unif(0,1)\nEstimators of central tendency can have very different properties.\nFor Unif(0,1), the mean, median, and midrange (defined by \\(\\frac{\\text{max} - \\text{min}}{2}\\)) are\nconsistent estimators of the center, but the median has high variance\nand the midrange is biased.\nSampling\nDistribution of the Mean for the Gamma Distribution\nAs sample size increases, the sampling distribution of the mean\ndecreases in variance\napproaches a normal distribution (central limit theorem)\nhas expected value equal to population mean\nFor example, mean of N i.i.d samples from Gamma(a,b) is distributed\nGamma(Na, Nb).\n\n\n\n",
      "last_modified": "2022-03-31T10:53:32-05:00"
    },
    {
      "path": "index.html",
      "title": "Recent Posts",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2022-03-31T10:53:34-05:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
