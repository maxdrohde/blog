{
  "articles": [
    {
      "path": "about.html",
      "title": "About me",
      "description": "",
      "author": [],
      "contents": "\nHello, I’m Max! I have a BA in Physics and Geology from Carleton College in Northfield, MN and I’m currently a Biostatistics graduate student at Vanderbilt University in Nashville, TN. Before coming to Vanderbilt, I worked as an ORISE Fellow at the FDA Division of Antiviral Products in the Office of New Drugs, where I analyzed clinical trials for hepatitis and HIV medications. My interests are in adaptive clinical trial design, reproducible research, and statistics education.\nIn my free time I enjoy cooking (and collecting cookbooks), playing funk guitar, and swimming.\nYou can find my publications at my Google scholar page here.\n\n\n\n",
      "last_modified": "2020-12-20T00:32:35-06:00"
    },
    {
      "path": "anim.html",
      "title": "Animations",
      "description": "",
      "author": [],
      "contents": "\nBelow are some animations I created using gganimate to help me understand concepts in statistics. The work of 3Blue1Brown and Seeing Theory have been an inspiration for me.\nBayesian Inference for Binomial Proportion\nBayesian inference can be using to quantity our uncertainty about the parameter \\(p\\) from a statistical model where the data is distributed \\(\\text{Bin}(n,p)\\). An example is the bias of a coin. In the below example, the true bias is \\(p = 0.7\\). As the data come in, we can update our posterior distribution for \\(p\\). \nBayesian Linear Regression\nBayesian inference can also be used in linear regression. As the data is collected, we become more confident about the parameters of the linear regression model:\nthe slope of the linear relationship\nthe variance of the error term\nthe intercept of the linear relationship\nThe model was fit using Stan to do the MCMC sampling and the posterior distributions were plotted using a kernel density estimate.\nOverfitting in Polynomial Regression\nOverfitting is a major problem when fitting complex models to few data points. As a simple example, polynomial regression can fit noise in the data rather than the true model. Here the true model is quadratic with error that is normally distributed with mean zero. As the degree of the polynomial increases, the model rapidly overfits the data. At the extreme, if the degree of the polynomial is greater than (Number of points - 1), then the fitted polynomial will pass through every data point. \nComparing Estimators for Unif(0, θ)\nThe maximum likelihood estimator (MLE), while it has many nice statistics properties, isn’t always the best estimator. To estimate θ for Unif(0,θ), the MLE is the maximum of the data, which is biased low. It is an underestimate of the true value of θ. To correct the bias, we can multiply the maximum by \\(\\frac{n+1}{n}\\) for an estimator that is unbiased, but has slightly higher variance. However, we see that the result is lower mean-squared error (MSE). We also show the method of moments estimator for reference, which is twice the sample mean. \nComparing Estimators for Center of Unif(0,1)\nEstimators of central tendency can have very different properties. For Unif(0,1), the mean, median, and midrange (defined by \\(\\frac{\\text{max} - \\text{min}}{2}\\)) are consistent estimators of the center, but the median has high variance and the midrange is biased. \nFinding the MLE estimate of the mean for a Normal Distribution\nYou can estimate the MLE for the mean of a normal distribution given a dataset by varying the mean until the peak of the log-likelihood is reached. This visually looks like sliding around different candidate distributions until the best match is found. Once the MLE for the mean is found, the variance can be varied in the same fashion (keeping the mean fixed at the MLE) to find the MLE for the variance. \nSampling Distribution of the Mean for the Gamma Distribution\nAs sample size increases, the sampling distribution of the mean\ndecreases in variance\napproaches a normal distribution (central limit theorem)\nhas expected value equal to population mean\nFor example, mean of N i.i.d samples from Gamma(a,b) is distributed Gamma(Na, Nb). \n\n\n\n",
      "last_modified": "2020-12-20T00:32:35-06:00"
    },
    {
      "path": "index.html",
      "title": "Recent Posts",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2020-12-20T00:32:35-06:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
